<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Making and Testing the Models</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">Home</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="index.html">Project Overview</a></li>
							<li><a href="the-data.html">The Data</a></li>
							<li class="active"><a href="model-development.html">The Model</a></li>
							<li><a href="other-projects.html">Other Projects</a></li>
							<li><a href="about.html">About Me</a></li>
						</ul>
						<ul class="icons">
							<li><a href="https://www.linkedin.com/in/mason-flannery/" class="icon brands fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>
							<li><a href="https://github.com/ddotsam" class="icon brands fa-github" target="_blank"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
				<div id="main">

					<!-- Body -->


					<!-- Post -->
						<section class="post">
							<header class="major">
								<h1>
									Making and Testing <br>the Models
								</h1>
							</header>

							<h2>
								Model Selection
							</h2>
							<p>
								Before discussing the models I ended up using, it is crucial to discuss the rationale behind the model selection process. 
								Several machine learning algorithms were considered, including logistic regression, random forests, k-NN, and gradient boosting methods.
								After careful evaluation and experimentation, two primary models were chosen: XGBoost and Stan's Bayesian logistic regression.
								Logistic regression was chosen for its ability to identify pertinent features clearly, and EDA revealed that the features met the assumptions.
								XGBoost was selected as a generally powerful binary classification algorithm, offering strong predictive capabilities.
								<span class="image right"><img src="images/XGB-feat-importance.jpeg" alt="Feature Importance graph for the XGBoost model" /></span>
							</p>

							<h2>
								Feature Selection
							</h2>
							<p>
								Selecting appropriate features is crucial for building effective predictive models. 
								In this project, a combination of domain knowledge and data exploration techniques, such as stepwise feature implementation, guided the feature selection process.
								The following features were included in the models:
								<ul>
									<li><b>Radiant Team GPM:</b> The average Gold Per Minute (GPM) of the Radiant team.</li>
									<li><b>Radiant Elo:</b> The Elo rating of the Radiant team, representing their skill level.</li>
									<li><b>Dire Team GPM:</b> The average GPM of the Dire team.</li>
									<li><b>Dire Elo:</b> The Elo rating of the Dire team.</li>
									<li><b>Tournament Type:</b> A categorical feature indicating the type of tournament (e.g., Major/Division 1, Division 2, Qualifier). 
										This feature was encoded using dummy variables to capture its effects in the models.</li>
								</ul>
								Each of the quantitative features was normalized based on their mean and standard deviation. 
								These features make sense because Elo is a measure of the skill of each team, and GPM is a measure of how the team gains advantages.
								<span class="image left"><img src="images/top-two-elo.jpeg" alt="Elo graphs of the top two teams" /></span>
								Interestingly, features that were initially expected to be useful, such as Experience Per Minute (XPM) and the range of GPM for each team, turned out to confuse the models.
								In fact, in the logistic regression model, these features had the opposite effect, with an increase in XPM leading to a slight decrease in win probability.
								After experimenting with various features, these five were chosen as they resulted in the models with the lowest LOOIC (leave-one-out cross-validation information criterion) for logistic regression. 
							</p>
							<h2>
								Model Comparisons and Insights
							</h2>
							<p>
								Both the XGBoost and logistic regression models provided interesting insights into the game including those discussed below:
								<ul>
									<li>
										The importance of GPM in determining match outcomes, which turned out to be far more insightful than XPM, contrary to initial expectations.
										<ul>
											<li>
												The GPM of the position 1 player, the GPM of the position 5 player, and the range between the two were all terrible predictors, showing the importance of team success as opposed to individual success.
												<span class="image right"><img src="images/dota-gold.jpg" alt="Gold Icon in Dota" /></span>
												This also came as a surprise because the GPM of position 1 players is paramount in my games: it seems like if the carry gets an early gold advantage, the other team has no chance of winning.
												However, the models would suggest that the wealth of the team is more important than that of the carry.
											</li>
											<li>
												Interestingly, the XPM of a team often yielded a negative coefficient: an increase in XPM lead to a decrease in win probability across all divisions.
											</li>
										</ul>
									</li>
									<li>
										The varying effects of different tournament types on match outcomes, with qualifiers exhibiting higher variance and major tournaments showcasing lower variance.
										<ul>
											<li>
												As expected, the model is less certain about matches that occur in qualifier rounds compared to division 1 rounds.
												This is because the breadth of skill in qualifiers is greater: everyone from the first team to be kicked out to the eventual TI winner must play in the qualifiers.
											</li>
											<li>
												As the division and general skill increased, both of the models tended to favor Dire over Radiant.
												In fact, when it comes to division 1 and major tournaments, the models generally predict a Dire win unless the Elo and GPM difference significantly favor the Radiant, showing a gameplay bias for the Dire side of the map. 
											</li>
										</ul>
									</li>
								</ul>
								By incorporating these findings, the models provide valuable insights for match prediction and analysis.
							</p>
							<h2>
								Results
							</h2>
							<p>
								Since the Elo for each team is updated at the end of every match, splitting the dataset into a train and test set would lead to data leakage. 
								This is because the model would attempt to predict the result of a game based on the teams' Elos, but those Elos already contain information on which team won the game since that game was considered in the calculation of the Elo.
								Thus, I felt it would be inappropriate to perform a train/test split for this dataset. 
								<span class="image left"><img src="images/bali-major.jpg" alt="Gold Icon in Dota" /></span>
							</p>
							<p>
								I will be testing the model's performance on the matches that occur during the Bali Major, which will take place from June 29th to July 9th.
								This is a prestigous tournament with a total prize pool of $500,000.
								Winning games in this tournamnt contribute significantly towards a team's qualification for TI, so competition will be fierce.
							</p>
						</section>

				</div>
				<!-- Footer -->
				<footer id="footer">
					<section class="split contact">
						<section class="alt">
							<h3>Address</h3>
							<p>1732 North 450 West #106<br />
							Provo, Utah 84604</p>
						</section>
						<section>
							<h3>Phone</h3>
							<p>(385) 208-6549</p>
						</section>
						<section>
							<h3>Email</h3>
							<p><a href="mailto:ddotsam@gmail.com">ddotsam@gmail.com</a></p>
						</section>
						<section>
							<h3>Social</h3>
							<ul class="icons alt">
								<li><a href="https://www.linkedin.com/in/mason-flannery/" class="icon brands alt fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>
								<li><a href="https://github.com/ddotsam" class="icon brands fa-github" target="_blank"><span class="label">GitHub</span></a></li>
							</ul>
						</section>
					</section>
				</footer>

			<!-- Copyright -->
				<div id="copyright">
					<ul><li>Design: <a href="https://html5up.net" target="_blank">HTML5 UP</a></li></ul>
				</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>